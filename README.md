# Cancer-Recognition-CNN
Convulted Neural Network that takes images from a database of cells both with and without cancer and both identifies and labels which images are which.
The breast histopathology image dataset used in this report holds significant relevance in the field of medical imaging and cancer research. By providing high-resolution microscopic images of breast tissue samples, this dataset offers a crucial resource for developing and evaluating machine learning algorithms aimed at automated cancer detection and diagnosis. With the capability to distinguish between benign and malignant tissue patterns, these images serve as a cornerstone for the development of computer-aided diagnostic systems, aiding medical professionals in accurately identifying and classifying breast cancer cases. Moreover, such datasets facilitate the exploration of novel image processing techniques and deep learning methodologies, contributing to advancements in early cancer detection, personalized treatment strategies, and ultimately, improved patient outcomes in the fight against breast cancer.
The code constructs a convolutional neural network (CNN) algorithm for image classification and utilizes the Feature Extraction layers which consists of several convolutional layers (Conv2D) followed by batch normalization (BatchNormalization) and max-pooling layers (MaxPooling2D). The convolutional layers apply filters to extract spatial features from the input images and batch normalization helps in stabilizing and accelerating the training process by normalizing the activations of each layer. Max-pooling layers help to reduce spatial dimensions while retaining important features in the feature maps. The size of the receptive field is 3x3 for each convolutional layer and there are 32, 64, 64, 128, and 256 feature maps in the respective convolutional layers. After running the neural network for 20 epochs on both the training and validation data, the FeatureExtractor is utilized by extracting the version of the model that performed the best out of each given run. Then, the code is updated to reflect the new model when evaluating the prediction accuracy on the test data. 
The architecture is also composed of cost and activation functions: the former is the Binary Crossentropy loss function that is indicated by tf.keras.losses.BinaryCrossentropy() in the compile() method. The activation functions: ReLU (Rectified Linear Unit) is used after each convolutional layer and Sigmoid is used in the output layer to produce binary classification probabilities (0 or 1) for the two classes. Each convolutional layer has parameters associated with its filters (kernels) and biases. The number of parameters in a convolutional layer is calculated as (filter_height * filter_width * input_channels + 1) * output_channels. The first convolutional layer has 32 filters with a kernel size of 3x3, so it contributes (3 * 3 * 3 + 1) * 32 parameters. Each dense (fully connected) layer has parameters associated with its weights and biases. The number of parameters in a dense layer is calculated as input_size * output_size + output_size. The first dense layer has 1000 neurons in the input and 512 neurons in the output, so it contributes (1000 * 512 + 512) parameters. 
The feedforward algorithm involves passing the input image through the network layer by layer, where each layer applies a transformation to the input data. In this architecture, the input image is passed through the FeatureExtractor layers, which extract top features from the input image. The extracted features are then flattened and passed through dense layers to learn representations. The network is then trained using the Stochastic Gradient Descent (SGD) optimization algorithm with backpropagation. During training, the model computes the loss (cost) between the predicted output and the ground truth labels. The optimizer (Adam optimizer in this case) adjusts the model's parameters (weights and biases) iteratively to minimize the loss function. Backpropagation is used to compute the gradients of the loss function with respect to each parameter in the network. The optimizer then updates the parameters in the direction that reduces the loss, effectively optimizing the network's performance.
	The dataset uses breast histopathology images, classified as 0 for “positive” indicating the breast tissue in the image is cancerous and 1 for “negative” respectively non-cancerous. In the original batch, I had a folder labeled “0” with 772 observations and a folder labeled “1” with 207. Since the number of samples for positive images is much more than for negative images, this leads the model to have a bias towards images classified under 0. This leads to the model having better accuracy when making a prediction about an image that is labeled under 0 as opposed to images labeled 1. Since I downloaded the dataset from kaggle, the full dataset includes images in a 3GB compressed zip folder, that is a large amount of data! For the purpose of this project, I extracted one random folder and iterated over the subdirectories of 0 and 1 within and created a pandas dataframe of the images with labels: “id”, “path”, and “target” that described the number of the image within the folder, the path, and the targeted classification respectively. I used different set ups to attempt to increase the accuracy of the model. The setups I used included:
The original, with 1 folder of data and 20 epochs of training. 
1 folder of data and 200 epochs of training. 
3 folders of data and 20 epochs of training. 
1 folder of data, but balanced each amount of 0 and 1 images to be roughly equal and trained for 20 epochs.
Each setup performed decently and I did see improvements in the accuracy from the original startup used in both the image prediction and the training and validation graphs. However, the other three set ups I used did not have significant improvements to the model, given that the original was already achieving an accuracy of 89% in the epoch training. I would say unsurprisingly, the best performing model was the more balanced dataset, given that there was such a large proportion discrepancy between the two types of images in every other setup. The balanced dataset is the one that allows the model the most chances to evaluate the images that are positive for cancer to train and test on. 
	Other than balancing the dataset more, another avenue that I think would be helpful to pursue with this data would be more image augmentation. It would be useful to train the model on versions of the same images that have been rotated perhaps 90 and 180 degrees, or even flipped vertically or horizontally. This would also expand the dataset exponentially, as there are many potential ways to orient an image differently to train the model to recognize. I believe this could also help to eliminate bias, so that the model would hopefully not treat the same image with a different diagnosis if the image is rotated, thus increasing the accuracy. With more time and resources, this could be a great route to pursue.
